{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c97dba-59f7-4cae-a8cb-1f6218850e6b",
   "metadata": {},
   "source": [
    "data for spy option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4debf2-3d26-4b17-be06-bde2bb1de265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "TICKER = \"SPY\"\n",
    "\n",
    "# Where to save the Excel file\n",
    "OUTPUT_PATH = r\"the file path for option\"\n",
    "\n",
    "# If you want ALL expirations, set MAX_EXPIRATIONS = None\n",
    "# If you only want the first N expirations (e.g. nearest 5), set an integer\n",
    "MAX_EXPIRATIONS = None  # or e.g. 5\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FUNCTION: download option chain\n",
    "# ============================================================\n",
    "\n",
    "def download_option_chain_for_ticker(ticker: str, max_exps=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download option chains from Yahoo Finance using yfinance for the given ticker.\n",
    "    Returns a single DataFrame with calls + puts for multiple expirations.\n",
    "    \"\"\"\n",
    "    print(f\"Downloading option chain metadata for {ticker}...\")\n",
    "    yf_ticker = yf.Ticker(ticker)\n",
    "\n",
    "    expirations = yf_ticker.options\n",
    "    if not expirations:\n",
    "        raise ValueError(f\"No option expirations found for {ticker}.\")\n",
    "\n",
    "    print(f\"Found {len(expirations)} expiration dates.\")\n",
    "\n",
    "    if max_exps is not None:\n",
    "        expirations = expirations[:max_exps]\n",
    "        print(f\"Using only first {len(expirations)} expirations: {expirations}\")\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for exp in expirations:\n",
    "        print(f\"Downloading option chain for expiration {exp}...\")\n",
    "        chain = yf_ticker.option_chain(exp)\n",
    "\n",
    "        calls = chain.calls.copy()\n",
    "        puts = chain.puts.copy()\n",
    "\n",
    "        # Add common metadata\n",
    "        calls[\"type\"] = \"call\"\n",
    "        puts[\"type\"] = \"put\"\n",
    "        calls[\"expiration\"] = exp\n",
    "        puts[\"expiration\"] = exp\n",
    "\n",
    "        all_rows.append(calls)\n",
    "        all_rows.append(puts)\n",
    "\n",
    "    print(\"Concatenating all expirations...\")\n",
    "    df_all = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "    # Reorder columns a bit\n",
    "    cols_order = [\n",
    "        \"type\", \"expiration\", \"contractSymbol\", \"lastTradeDate\",\n",
    "        \"strike\", \"lastPrice\", \"bid\", \"ask\",\n",
    "        \"change\", \"percentChange\", \"volume\", \"openInterest\",\n",
    "        \"impliedVolatility\", \"inTheMoney\", \"contractSize\", \"currency\"\n",
    "    ]\n",
    "    existing_cols = [c for c in cols_order if c in df_all.columns]\n",
    "    df_all = df_all[existing_cols]\n",
    "\n",
    "    # Convert to datetime\n",
    "    if \"lastTradeDate\" in df_all.columns:\n",
    "        df_all[\"lastTradeDate\"] = pd.to_datetime(df_all[\"lastTradeDate\"], errors=\"coerce\")\n",
    "    df_all[\"expiration\"] = pd.to_datetime(df_all[\"expiration\"], errors=\"coerce\")\n",
    "\n",
    "    # ⚠ IMPORTANT: drop timezones so Excel is happy\n",
    "    for col in df_all.columns:\n",
    "        if pd.api.types.is_datetime64tz_dtype(df_all[col]):\n",
    "            df_all[col] = df_all[col].dt.tz_convert(None)\n",
    "\n",
    "    return df_all\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN: download + save to Excel\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(f\"Starting options download for {TICKER}...\")\n",
    "    df_options = download_option_chain_for_ticker(TICKER, max_exps=MAX_EXPIRATIONS)\n",
    "\n",
    "    print(f\"Downloaded {len(df_options)} rows of options data.\")\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    out_dir = os.path.dirname(OUTPUT_PATH)\n",
    "    if out_dir and not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Saving to Excel: {OUTPUT_PATH}\")\n",
    "    with pd.ExcelWriter(OUTPUT_PATH, engine=\"openpyxl\") as writer:\n",
    "        df_options.to_excel(writer, sheet_name=\"SPY_Options\", index=False)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f984ba3d-b848-4716-a8b2-33b374d5d984",
   "metadata": {},
   "source": [
    "excel for spy stock price is in another file, since its "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4bfd61-e9be-4c59-95c4-f2f31d3e16a1",
   "metadata": {},
   "source": [
    "code for model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed541c-7740-40eb-a3a0-e0d84d271a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Backtesting 4 models on SPY using Excel data, with Heston regime mixture\n",
    "calibrated (partially) from SPY option-implied volatilities.\n",
    "\n",
    "Models:\n",
    "  2-0: data -> HMM (fit once, frozen)      -> DKL vs empirical\n",
    "  2-1: data -> HMM (refit up to t)         -> DKL vs empirical\n",
    "  2-2: data -> local-vol-like Gaussian     -> DKL vs empirical\n",
    "  2-3: data -> HMM, Heston, Heston-mixture (with IV-based theta) -> pick min DKL model\n",
    "\n",
    "Files:\n",
    "\n",
    "  Price file (daily SPY):\n",
    "    C:\\\\Users\\\\brian\\\\OneDrive\\\\桌面\\\\algorithmic trading\\\\data.xlsx\n",
    "    Columns (case-insensitive): date, adj close, ...\n",
    "\n",
    "  Options file (SPY options from Yahoo Finance script):\n",
    "    C:\\\\Users\\\\brian\\\\OneDrive\\\\桌面\\\\algorithmic trading\\\\options_data.xlsx\n",
    "    Sheet: SPY_Options\n",
    "    Columns include: type, expiration, strike, lastPrice, bid, ask,\n",
    "                     impliedVolatility, lastTradeDate, etc.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Load SPY daily prices (underlying + benchmark)\n",
    "# ============================================================\n",
    "\n",
    "def load_excel_prices_spy(filepath: str):\n",
    "    \"\"\"\n",
    "    Load your SPY Excel file.\n",
    "\n",
    "    Expected columns: 'date', 'adj close' (case-insensitive).\n",
    "    We treat 'adj close' as both:\n",
    "      - underlying asset price\n",
    "      - SPY benchmark price.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(filepath)\n",
    "\n",
    "    # Normalize date column\n",
    "    date_col = None\n",
    "    for c in df.columns:\n",
    "        if \"date\" in c.lower():\n",
    "            date_col = c\n",
    "            break\n",
    "    if date_col is None:\n",
    "        raise ValueError(\"No date column found (expected something like 'date').\")\n",
    "\n",
    "    df = df.rename(columns={date_col: \"Date\"})\n",
    "\n",
    "    # Find adj close column\n",
    "    adj_col = None\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if \"adj\" in lc and \"close\" in lc:\n",
    "            adj_col = c\n",
    "            break\n",
    "    if adj_col is None:\n",
    "        raise ValueError(\"No 'adj close' column found.\")\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df.sort_values(\"Date\").set_index(\"Date\")\n",
    "\n",
    "    # Use adj close as underlying and SPY benchmark\n",
    "    df = df[[adj_col]].copy()\n",
    "    df.columns = [\"underlying\"]\n",
    "    df[\"spy\"] = df[\"underlying\"]\n",
    "\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Load SPY options (from options_data.xlsx)\n",
    "# ============================================================\n",
    "\n",
    "def load_spy_options(filepath: str, sheet_name: str = \"SPY_Options\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load SPY options data produced by the Yahoo Finance downloader script.\n",
    "\n",
    "    Expected columns (subset):\n",
    "      type, expiration, contractSymbol, lastTradeDate, strike,\n",
    "      lastPrice, bid, ask, impliedVolatility, volume, openInterest, ...\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Options file not found: {filepath}\")\n",
    "\n",
    "    df_opt = pd.read_excel(filepath, sheet_name=sheet_name)\n",
    "\n",
    "    # Parse datetimes\n",
    "    if \"expiration\" in df_opt.columns:\n",
    "        df_opt[\"expiration\"] = pd.to_datetime(df_opt[\"expiration\"], errors=\"coerce\")\n",
    "    if \"lastTradeDate\" in df_opt.columns:\n",
    "        df_opt[\"lastTradeDate\"] = pd.to_datetime(df_opt[\"lastTradeDate\"], errors=\"coerce\")\n",
    "\n",
    "    # Clean impliedVolatility: it is in decimal (e.g. 0.15 for 15%)\n",
    "    if \"impliedVolatility\" not in df_opt.columns:\n",
    "        raise ValueError(\"Options file must have 'impliedVolatility' column.\")\n",
    "\n",
    "    df_opt = df_opt.dropna(subset=[\"expiration\", \"impliedVolatility\"])\n",
    "    return df_opt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Basic utilities\n",
    "# ============================================================\n",
    "\n",
    "def log_returns(series: pd.Series) -> pd.Series:\n",
    "    return np.log(series / series.shift(1)).dropna()\n",
    "\n",
    "\n",
    "def rolling_empirical_distribution(\n",
    "    returns: pd.Series,\n",
    "    t_index: int,\n",
    "    H: int,\n",
    "    lookback: int,\n",
    "    n_bins: int = 30\n",
    "):\n",
    "    \"\"\"\n",
    "    Build empirical distribution of H-day log-returns at time t_index\n",
    "    using a rolling window of length `lookback`.\n",
    "    \"\"\"\n",
    "    start = max(0, t_index - lookback - H)\n",
    "    end = t_index - 1\n",
    "    if end - start <= H:\n",
    "        return None, None\n",
    "\n",
    "    H_returns = []\n",
    "    for k in range(start, end - H + 2):\n",
    "        block = returns.iloc[k:k+H]\n",
    "        H_returns.append(block.sum())\n",
    "    H_returns = np.array(H_returns)\n",
    "\n",
    "    if len(H_returns) < 10:\n",
    "        return None, None\n",
    "\n",
    "    hist, edges = np.histogram(H_returns, bins=n_bins, density=False)\n",
    "    hist = hist.astype(float)\n",
    "    if hist.sum() == 0:\n",
    "        return None, None\n",
    "    p_emp = hist / hist.sum()\n",
    "    bin_centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    return bin_centers, p_emp\n",
    "\n",
    "\n",
    "def discrete_kl(p, q, eps=1e-12):\n",
    "    \"\"\"\n",
    "    KL(p||q) for discrete distributions p, q given as arrays on same bins.\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    q = np.clip(q, eps, 1.0)\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Gaussian HMM (univariate returns)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class GaussianHMM:\n",
    "    n_states: int = 2\n",
    "    tol: float = 1e-5\n",
    "    max_iter: int = 200\n",
    "    random_state: int = 42\n",
    "    verbose: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.rng = np.random.default_rng(self.random_state)\n",
    "        self.pi_ = None\n",
    "        self.A_ = None\n",
    "        self.mu_ = None\n",
    "        self.sigma2_ = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _logsumexp(x, axis=None):\n",
    "        m = np.max(x, axis=axis, keepdims=True)\n",
    "        s = np.log(np.sum(np.exp(x - m), axis=axis, keepdims=True)) + m\n",
    "        return np.squeeze(s, axis=axis)\n",
    "\n",
    "    @staticmethod\n",
    "    def _lognorm_pdf(x, mu, sigma2):\n",
    "        return -0.5 * np.log(2 * np.pi * sigma2) - 0.5 * (x - mu) ** 2 / sigma2\n",
    "\n",
    "    def _init_params(self, x):\n",
    "        K = self.n_states\n",
    "        T = len(x)\n",
    "        self.pi_ = np.ones(K) / K\n",
    "        A = np.eye(K) * 0.9\n",
    "        A[A == 0] = 0.1 / (K - 1)\n",
    "        self.A_ = A\n",
    "\n",
    "        # simple init with k-means-like assignment\n",
    "        q = self.rng.integers(0, K, size=T)\n",
    "        for _ in range(3):\n",
    "            mus = np.array([x[q == k].mean() if np.any(q == k) else x.mean() for k in range(K)])\n",
    "            q = np.argmin(np.abs(x[:, None] - mus[None, :]), axis=1)\n",
    "        self.mu_ = np.array([x[q == k].mean() if np.any(q == k) else x.mean() for k in range(K)])\n",
    "        self.sigma2_ = np.array([x[q == k].var() if np.any(q == k) else x.var() for k in range(K)])\n",
    "        self.sigma2_[self.sigma2_ <= 1e-10] = 1e-10\n",
    "\n",
    "    def _emission_logprob(self, x):\n",
    "        T = len(x)\n",
    "        logB = np.zeros((T, self.n_states))\n",
    "        for k in range(self.n_states):\n",
    "            logB[:, k] = self._lognorm_pdf(x, self.mu_[k], self.sigma2_[k])\n",
    "        return logB\n",
    "\n",
    "    def _forward(self, logB):\n",
    "        T, K = logB.shape\n",
    "        logA = np.log(self.A_)\n",
    "        logpi = np.log(self.pi_)\n",
    "        log_alpha = np.zeros((T, K))\n",
    "        c = np.zeros(T)\n",
    "\n",
    "        log_alpha[0] = logpi + logB[0]\n",
    "        c[0] = self._logsumexp(log_alpha[0])\n",
    "        log_alpha[0] -= c[0]\n",
    "        for t in range(1, T):\n",
    "            for j in range(K):\n",
    "                log_alpha[t, j] = self._logsumexp(log_alpha[t-1] + logA[:, j]) + logB[t, j]\n",
    "            c[t] = self._logsumexp(log_alpha[t])\n",
    "            log_alpha[t] -= c[t]\n",
    "        loglik = np.sum(c)\n",
    "        return log_alpha, c, loglik\n",
    "\n",
    "    def _backward(self, logB, c):\n",
    "        T, K = logB.shape\n",
    "        logA = np.log(self.A_)\n",
    "        log_beta = np.zeros((T, K))\n",
    "        log_beta[-1] = -c[-1]\n",
    "        for t in range(T-2, -1, -1):\n",
    "            for i in range(K):\n",
    "                log_beta[t, i] = self._logsumexp(logA[i] + logB[t+1] + log_beta[t+1])\n",
    "            log_beta[t] -= c[t]\n",
    "        return log_beta\n",
    "\n",
    "    def _e_step(self, x):\n",
    "        logB = self._emission_logprob(x)\n",
    "        log_alpha, c, loglik = self._forward(logB)\n",
    "        log_beta = self._backward(logB, c)\n",
    "        log_gamma = log_alpha + log_beta\n",
    "        log_gamma -= self._logsumexp(log_gamma, axis=1)[:, None]\n",
    "        gamma = np.exp(log_gamma)\n",
    "\n",
    "        T, K = gamma.shape\n",
    "        logA = np.log(self.A_)\n",
    "        log_xi = np.zeros((T-1, K, K))\n",
    "        for t in range(T-1):\n",
    "            M = (\n",
    "                log_alpha[t][:, None]\n",
    "                + logA\n",
    "                + logB[t+1][None, :]\n",
    "                + log_beta[t+1][None, :]\n",
    "            )\n",
    "            M -= self._logsumexp(M)\n",
    "            log_xi[t] = M\n",
    "        xi = np.exp(log_xi)\n",
    "        return gamma, xi, loglik\n",
    "\n",
    "    def _m_step(self, x, gamma, xi):\n",
    "        T, K = gamma.shape\n",
    "        self.pi_ = gamma[0] / gamma[0].sum()\n",
    "        xi_sum = xi.sum(axis=0)\n",
    "        gamma_sum = gamma[:-1].sum(axis=0)\n",
    "        self.A_ = xi_sum / np.maximum(gamma_sum[:, None], 1e-16)\n",
    "        self.A_ = np.clip(self.A_, 1e-12, None)\n",
    "        self.A_ /= self.A_.sum(axis=1, keepdims=True)\n",
    "\n",
    "        for k in range(K):\n",
    "            w = gamma[:, k]\n",
    "            wsum = w.sum()\n",
    "            if wsum <= 1e-16:\n",
    "                continue\n",
    "            mu = np.sum(w * x) / wsum\n",
    "            var = np.sum(w * (x - mu) ** 2) / wsum\n",
    "            self.mu_[k] = mu\n",
    "            self.sigma2_[k] = max(var, 1e-12)\n",
    "\n",
    "    def fit(self, x):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        if self.pi_ is None:\n",
    "            self._init_params(x)\n",
    "        prev = -np.inf\n",
    "        for it in range(self.max_iter):\n",
    "            gamma, xi, loglik = self._e_step(x)\n",
    "            self._m_step(x, gamma, xi)\n",
    "            if self.verbose:\n",
    "                print(f\"EM iter {it+1:3d}: loglik={loglik:.6f}\")\n",
    "            if abs(loglik - prev) < self.tol:\n",
    "                break\n",
    "            prev = loglik\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        logB = self._emission_logprob(x)\n",
    "        log_alpha, c, _ = self._forward(logB)\n",
    "        log_beta = self._backward(logB, c)\n",
    "        log_gamma = log_alpha + log_beta\n",
    "        log_gamma -= self._logsumexp(log_gamma, axis=1)[:, None]\n",
    "        return np.exp(log_gamma)\n",
    "\n",
    "    def next_horizon_distribution(self, x, H=5, n_sims=300, n_bins=30):\n",
    "        \"\"\"\n",
    "        Monte Carlo distribution of H-day returns from fitted HMM.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        gamma = self.predict_proba(x)\n",
    "        p_ST = gamma[-1]  # last-day posteriors\n",
    "\n",
    "        K = self.n_states\n",
    "        H_returns = []\n",
    "        for _ in range(n_sims):\n",
    "            # sample next state from p_ST @ A\n",
    "            p_next = p_ST @ self.A_\n",
    "            state = np.argmax(np.random.multinomial(1, p_next))\n",
    "            total = 0.0\n",
    "            for _h in range(H):\n",
    "                r = np.random.normal(self.mu_[state], np.sqrt(self.sigma2_[state]))\n",
    "                total += r\n",
    "                state = np.argmax(np.random.multinomial(1, self.A_[state]))\n",
    "            H_returns.append(total)\n",
    "\n",
    "        H_returns = np.array(H_returns)\n",
    "        hist, edges = np.histogram(H_returns, bins=n_bins, density=False)\n",
    "        hist = hist.astype(float)\n",
    "        if hist.sum() == 0:\n",
    "            return None, None\n",
    "        q = hist / hist.sum()\n",
    "        centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "        return centers, q\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Simple \"local volatility\" model\n",
    "# ============================================================\n",
    "\n",
    "def local_vol_distribution(\n",
    "    returns: pd.Series,\n",
    "    t_index: int,\n",
    "    H: int,\n",
    "    lookback: int,\n",
    "    n_sims: int = 300,\n",
    "    n_bins: int = 30\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple local-vol style model:\n",
    "      - Estimate mean and std of H-day returns in rolling lookback window.\n",
    "      - Assume Normal with those parameters and generate samples.\n",
    "      - Histogram -> model-implied distribution.\n",
    "    \"\"\"\n",
    "    start = max(0, t_index - lookback - H)\n",
    "    end = t_index - 1\n",
    "    if end - start <= H:\n",
    "        return None, None\n",
    "\n",
    "    H_rets = []\n",
    "    for k in range(start, end - H + 2):\n",
    "        H_rets.append(returns.iloc[k:k+H].sum())\n",
    "    H_rets = np.array(H_rets)\n",
    "    if len(H_rets) < 10:\n",
    "        return None, None\n",
    "\n",
    "    mu = H_rets.mean()\n",
    "    sigma = H_rets.std(ddof=1)\n",
    "    if sigma <= 1e-8:\n",
    "        sigma = 1e-8\n",
    "\n",
    "    sims = np.random.normal(mu, sigma, size=n_sims)\n",
    "    hist, edges = np.histogram(sims, bins=n_bins, density=False)\n",
    "    hist = hist.astype(float)\n",
    "    if hist.sum() == 0:\n",
    "        return None, None\n",
    "    q = hist / hist.sum()\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    return centers, q\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Heston simulation + IV-based calibration\n",
    "# ============================================================\n",
    "\n",
    "def simulate_heston_H_returns(\n",
    "    H: int,\n",
    "    n_sims: int,\n",
    "    mu: float,\n",
    "    kappa: float,\n",
    "    theta: float,\n",
    "    xi: float,\n",
    "    rho: float,\n",
    "    v0: float,\n",
    "    dt: float = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple Euler Heston simulation for H-day log-returns.\n",
    "    We simulate S_t with S_0 = 1 and output log(S_H).\n",
    "    \"\"\"\n",
    "    H_returns = []\n",
    "    for _ in range(n_sims):\n",
    "        S = 1.0\n",
    "        v = max(v0, 1e-8)\n",
    "        for _h in range(H):\n",
    "            z1 = np.random.normal()\n",
    "            z2 = np.random.normal()\n",
    "            # Correlated Brownian motions\n",
    "            z2_corr = rho * z1 + np.sqrt(max(1.0 - rho**2, 0.0)) * z2\n",
    "            dW1 = np.sqrt(dt) * z1\n",
    "            dW2 = np.sqrt(dt) * z2_corr\n",
    "\n",
    "            # variance process\n",
    "            v = v + kappa * (theta - v) * dt + xi * np.sqrt(max(v, 0.0)) * dW2\n",
    "            v = max(v, 1e-10)\n",
    "\n",
    "            # price process\n",
    "            S = S * np.exp((mu - 0.5 * v) * dt + np.sqrt(v) * dW1)\n",
    "\n",
    "        H_returns.append(np.log(S))\n",
    "    return np.array(H_returns)\n",
    "\n",
    "\n",
    "def calibrate_theta_from_options(\n",
    "    df_opt: pd.DataFrame,\n",
    "    as_of_date: pd.Timestamp,\n",
    "    min_days: int = 20,\n",
    "    max_days: int = 90\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Very simple IV-based calibration:\n",
    "\n",
    "      1. Compute days_to_exp = expiration - as_of_date.\n",
    "      2. Select options with min_days <= days_to_exp <= max_days.\n",
    "      3. Compute average implied volatility sigma_IV.\n",
    "      4. Set theta = sigma_IV^2 (annual variance level).\n",
    "\n",
    "    If no suitable options found, returns None.\n",
    "    \"\"\"\n",
    "    # Ensure date is naive\n",
    "    as_of_date = pd.to_datetime(as_of_date).normalize()\n",
    "\n",
    "    df = df_opt.copy()\n",
    "    df[\"days_to_exp\"] = (df[\"expiration\"].dt.normalize() - as_of_date).dt.days\n",
    "\n",
    "    mask = (df[\"days_to_exp\"] >= min_days) & (df[\"days_to_exp\"] <= max_days)\n",
    "    df_slice = df.loc[mask]\n",
    "\n",
    "    if len(df_slice) == 0:\n",
    "        return None\n",
    "\n",
    "    # Filter out insane IVs\n",
    "    iv = df_slice[\"impliedVolatility\"].astype(float)\n",
    "    iv = iv[(iv > 0.0) & (iv < 5.0)]  # 0% < IV < 500%\n",
    "    if len(iv) == 0:\n",
    "        return None\n",
    "\n",
    "    sigma_iv = iv.mean()  # annualized volatility (per year)\n",
    "    theta = sigma_iv ** 2\n",
    "    return float(theta)\n",
    "\n",
    "\n",
    "def heston_regime_mixture_distribution(\n",
    "    hmm: GaussianHMM,\n",
    "    x_hist: np.ndarray,\n",
    "    H: int,\n",
    "    kappa: float,\n",
    "    theta: float,\n",
    "    xi: float,\n",
    "    rho: float,\n",
    "    r_annual: float = 0.02,\n",
    "    n_sims: int = 300,\n",
    "    n_bins: int = 30\n",
    "):\n",
    "    \"\"\"\n",
    "    Build mixture-of-Heston-regimes distribution:\n",
    "\n",
    "      q_mix(x) = sum_k p_k q_Heston^{(k)}(x),\n",
    "\n",
    "    where p_k are HMM posterior probabilities at current time,\n",
    "    and each regime k has its own initial variance v0_k derived\n",
    "    from the HMM state variance (v0_k ~ 252 * sigma2_k).\n",
    "\n",
    "    Heston parameters (kappa, theta, xi, rho) are provided by\n",
    "    an IV-based calibration from options.\n",
    "    \"\"\"\n",
    "    # HMM posteriors for history\n",
    "    gamma = hmm.predict_proba(x_hist)\n",
    "    p_regime = gamma[-1]  # shape (K,)\n",
    "    K = hmm.n_states\n",
    "\n",
    "    # Map from HMM variance to Heston initial variance per regime\n",
    "    v0_list = 252.0 * hmm.sigma2_.copy()\n",
    "    v0_list[v0_list <= 1e-10] = 1e-10\n",
    "\n",
    "    mu = r_annual / 252.0  # daily drift\n",
    "\n",
    "    # Simulate regime-specific Heston distributions\n",
    "    samples_per_regime = []\n",
    "    for k in range(K):\n",
    "        H_rets_k = simulate_heston_H_returns(\n",
    "            H=H,\n",
    "            n_sims=n_sims,\n",
    "            mu=mu,\n",
    "            kappa=kappa,\n",
    "            theta=theta,\n",
    "            xi=xi,\n",
    "            rho=rho,\n",
    "            v0=v0_list[k],\n",
    "            dt=1.0\n",
    "        )\n",
    "        samples_per_regime.append(H_rets_k)\n",
    "\n",
    "    # Build common histogram edges\n",
    "    all_samps = np.concatenate(samples_per_regime)\n",
    "    low = np.percentile(all_samps, 0.5)\n",
    "    high = np.percentile(all_samps, 99.5)\n",
    "    if low == high:\n",
    "        low -= 1e-4\n",
    "        high += 1e-4\n",
    "    edges = np.linspace(low, high, n_bins + 1)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "    # Hist per regime\n",
    "    q_k = []\n",
    "    for k in range(K):\n",
    "        hist_k, _ = np.histogram(samples_per_regime[k], bins=edges, density=False)\n",
    "        hist_k = hist_k.astype(float)\n",
    "        if hist_k.sum() == 0:\n",
    "            hist_k = np.ones_like(hist_k, dtype=float)\n",
    "        q_k.append(hist_k / hist_k.sum())\n",
    "    q_k = np.stack(q_k, axis=0)  # shape (K, n_bins)\n",
    "\n",
    "    # Mixture using HMM posterior weights p_regime\n",
    "    q_mix = np.zeros_like(q_k[0])\n",
    "    for k in range(K):\n",
    "        q_mix += p_regime[k] * q_k[k]\n",
    "    q_mix = np.clip(q_mix, 1e-12, None)\n",
    "    q_mix /= q_mix.sum()\n",
    "    return centers, q_mix\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Backtesting framework\n",
    "# ============================================================\n",
    "\n",
    "def align_and_kl(bin_emp, p_emp, bin_model, q_model):\n",
    "    \"\"\"\n",
    "    Interpolate model distribution q_model onto empirical bin grid\n",
    "    and compute KL(p_emp || q_model_interp).\n",
    "    \"\"\"\n",
    "    if bin_emp is None or p_emp is None or bin_model is None or q_model is None:\n",
    "        return None\n",
    "    q_interp = np.interp(bin_emp, bin_model, q_model, left=1e-12, right=1e-12)\n",
    "    q_interp = np.clip(q_interp, 1e-12, None)\n",
    "    q_interp = q_interp / q_interp.sum()\n",
    "    return discrete_kl(p_emp, q_interp)\n",
    "\n",
    "\n",
    "def backtest_model(\n",
    "    df: pd.DataFrame,\n",
    "    df_opt: pd.DataFrame,\n",
    "    model_mode: str,\n",
    "    H: int = 5,\n",
    "    lookback: int = 252,\n",
    "    n_bins: int = 30,\n",
    "    n_sims: int = 300,\n",
    "    hmm_states: int = 2,\n",
    "    r_annual: float = 0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    Backtest a model mode: '2-0', '2-1', '2-2', '2-3'.\n",
    "\n",
    "    Trading rule (simple):\n",
    "      - Compute model-implied expected H-day return E[R] from distribution.\n",
    "      - If E[R] > 0 and KL(p_emp || q_model) <= tau: go long underlying for 1 day.\n",
    "      - Else: stay flat.\n",
    "    \"\"\"\n",
    "    r_u = log_returns(df[\"underlying\"])\n",
    "    r_spy = log_returns(df[\"spy\"])\n",
    "\n",
    "    # align length\n",
    "    r_spy = r_spy.reindex(r_u.index).dropna()\n",
    "    r_u = r_u.reindex(r_spy.index)\n",
    "\n",
    "    idx = r_u.index\n",
    "    n = len(r_u)\n",
    "\n",
    "    # initial HMM for mode 2-0\n",
    "    if model_mode == \"2-0\":\n",
    "        hmm = GaussianHMM(n_states=hmm_states, verbose=False)\n",
    "        train_end = min(lookback, n - H - 1)\n",
    "        hmm.fit(r_u.iloc[:train_end].values)\n",
    "\n",
    "    equity = [1.0]\n",
    "    spy_equity = [1.0]\n",
    "    actions = []\n",
    "\n",
    "    for t_index in range(lookback, n - H):\n",
    "        # progress print every 50 steps\n",
    "        if (t_index - lookback) % 50 == 0:\n",
    "            print(f\"{model_mode}: step {t_index - lookback} / {n - lookback - H}\")\n",
    "\n",
    "        # empirical distribution at time t_index\n",
    "        bin_emp, p_emp = rolling_empirical_distribution(\n",
    "            r_u, t_index, H=H, lookback=lookback, n_bins=n_bins\n",
    "        )\n",
    "\n",
    "        if bin_emp is None:\n",
    "            actions.append(0)\n",
    "            next_u = equity[-1] * np.exp(0.0)\n",
    "            next_spy = spy_equity[-1] * np.exp(r_spy.iloc[t_index])\n",
    "            equity.append(next_u)\n",
    "            spy_equity.append(next_spy)\n",
    "            continue\n",
    "\n",
    "        r_next = r_u.iloc[t_index]\n",
    "        current_date = idx[t_index]\n",
    "\n",
    "        q_model = None\n",
    "        bin_model = None\n",
    "\n",
    "        # Model 2-0: single HMM fit, frozen\n",
    "        if model_mode == \"2-0\":\n",
    "            bin_model, q_model = hmm.next_horizon_distribution(\n",
    "                r_u.iloc[:t_index].values, H=H, n_sims=n_sims, n_bins=n_bins\n",
    "            )\n",
    "\n",
    "        # Model 2-1: re-fit HMM each step on all past data\n",
    "        elif model_mode == \"2-1\":\n",
    "            hmm_step = GaussianHMM(n_states=hmm_states, verbose=False)\n",
    "            hmm_step.fit(r_u.iloc[:t_index].values)\n",
    "            bin_model, q_model = hmm_step.next_horizon_distribution(\n",
    "                r_u.iloc[:t_index].values, H=H, n_sims=n_sims, n_bins=n_bins\n",
    "            )\n",
    "\n",
    "        # Model 2-2: local-vol style model on H-day returns\n",
    "        elif model_mode == \"2-2\":\n",
    "            bin_model, q_model = local_vol_distribution(\n",
    "                r_u, t_index, H=H, lookback=lookback, n_sims=n_sims, n_bins=n_bins\n",
    "            )\n",
    "\n",
    "        # Model 2-3: HMM, Heston (IV-calibrated), and Heston-mixture -> choose by min KL\n",
    "        elif model_mode == \"2-3\":\n",
    "            # HMM distribution\n",
    "            hmm_step = GaussianHMM(n_states=hmm_states, verbose=False)\n",
    "            hmm_step.fit(r_u.iloc[:t_index].values)\n",
    "            x_hist = r_u.iloc[:t_index].values\n",
    "            bin_hmm, q_hmm = hmm_step.next_horizon_distribution(\n",
    "                x_hist, H=H, n_sims=n_sims, n_bins=n_bins\n",
    "            )\n",
    "\n",
    "            # Single local-vol approximation\n",
    "            bin_heston_local, q_heston_local = local_vol_distribution(\n",
    "                r_u, t_index, H=H, lookback=lookback, n_sims=n_sims, n_bins=n_bins\n",
    "            )\n",
    "\n",
    "            # Calibrate theta from options for this date (rough, uses near-term maturity)\n",
    "            theta_est = calibrate_theta_from_options(df_opt, current_date, min_days=20, max_days=90)\n",
    "            if theta_est is None:\n",
    "                # fallback: use HMM-derived variance\n",
    "                theta_est = float(252.0 * np.mean(hmm_step.sigma2_))\n",
    "\n",
    "            # Heston hyperparameters (can be tuned)\n",
    "            kappa = 1.5\n",
    "            xi = 0.5\n",
    "            rho = -0.5\n",
    "\n",
    "            # Heston regime mixture\n",
    "            bin_mix, q_mix = heston_regime_mixture_distribution(\n",
    "                hmm_step,\n",
    "                x_hist,\n",
    "                H=H,\n",
    "                kappa=kappa,\n",
    "                theta=theta_est,\n",
    "                xi=xi,\n",
    "                rho=rho,\n",
    "                r_annual=r_annual,\n",
    "                n_sims=n_sims,\n",
    "                n_bins=n_bins\n",
    "            )\n",
    "\n",
    "            # KLs on empirical bins\n",
    "            kl_hmm = align_and_kl(bin_emp, p_emp, bin_hmm, q_hmm)\n",
    "            kl_heston_local = align_and_kl(bin_emp, p_emp, bin_heston_local, q_heston_local)\n",
    "            kl_mix = align_and_kl(bin_emp, p_emp, bin_mix, q_mix)\n",
    "\n",
    "            # Choose the model with smallest KL\n",
    "            kl_list = []\n",
    "            model_list = []\n",
    "\n",
    "            if kl_hmm is not None:\n",
    "                kl_list.append(kl_hmm)\n",
    "                model_list.append((\"HMM\", bin_hmm, q_hmm))\n",
    "            if kl_heston_local is not None:\n",
    "                kl_list.append(kl_heston_local)\n",
    "                model_list.append((\"HESTON_LOCAL\", bin_heston_local, q_heston_local))\n",
    "            if kl_mix is not None:\n",
    "                kl_list.append(kl_mix)\n",
    "                model_list.append((\"HESTON_MIX\", bin_mix, q_mix))\n",
    "\n",
    "            if len(kl_list) == 0:\n",
    "                bin_model, q_model = None, None\n",
    "            else:\n",
    "                best_idx = int(np.argmin(kl_list))\n",
    "                _, bin_model, q_model = model_list[best_idx]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_mode: {model_mode}\")\n",
    "\n",
    "        kl_val = align_and_kl(bin_emp, p_emp, bin_model, q_model)\n",
    "\n",
    "        # expected H-day return from q_model\n",
    "        if (bin_model is None) or (q_model is None):\n",
    "            exp_H = 0.0\n",
    "        else:\n",
    "            exp_H = np.sum(bin_model * q_model)\n",
    "\n",
    "        tau = 0.2  # KL threshold; tune as needed\n",
    "        if (kl_val is not None) and (kl_val <= tau) and (exp_H > 0.0):\n",
    "            position = 1.0\n",
    "        else:\n",
    "            position = 0.0\n",
    "\n",
    "        actions.append(position)\n",
    "\n",
    "        new_equity = equity[-1] * np.exp(position * r_next)\n",
    "        equity.append(new_equity)\n",
    "\n",
    "        new_spy = spy_equity[-1] * np.exp(r_spy.iloc[t_index])\n",
    "        spy_equity.append(new_spy)\n",
    "\n",
    "    # stop at n - H (exclusive) to match equity length\n",
    "    res_index = idx[lookback:n - H]\n",
    "    equity_series = pd.Series(equity[1:], index=res_index, name=f\"model_{model_mode}\")\n",
    "    spy_series = pd.Series(spy_equity[1:], index=res_index, name=\"SPY_benchmark\")\n",
    "    actions_series = pd.Series(actions, index=res_index, name=f\"action_{model_mode}\")\n",
    "    return equity_series, spy_series, actions_series\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Run all 4 models\n",
    "# ============================================================\n",
    "\n",
    "def run_all_models(price_path: str, options_path: str):\n",
    "    df = load_excel_prices_spy(price_path)\n",
    "    df_opt = load_spy_options(options_path, sheet_name=\"SPY_Options\")\n",
    "\n",
    "    H = 5\n",
    "    lookback = 252  # 1 year for empirical estimation\n",
    "\n",
    "    equity_dict = {}\n",
    "    spy_ref = None\n",
    "\n",
    "    for mode in [\"2-0\", \"2-1\", \"2-2\", \"2-3\"]:\n",
    "        print(f\"Starting backtest for model {mode}...\")\n",
    "        eq, spy, act = backtest_model(\n",
    "            df,\n",
    "            df_opt,\n",
    "            model_mode=mode,\n",
    "            H=H,\n",
    "            lookback=lookback,\n",
    "            n_bins=30,\n",
    "            n_sims=300,   # <= here: 300 sims per step\n",
    "            hmm_states=2,\n",
    "            r_annual=0.02\n",
    "        )\n",
    "        equity_dict[mode] = eq\n",
    "        if spy_ref is None:\n",
    "            spy_ref = spy\n",
    "        print(f\"Finished backtest for model {mode}.\")\n",
    "\n",
    "    all_df = pd.concat(\n",
    "        [equity_dict[\"2-0\"], equity_dict[\"2-1\"],\n",
    "         equity_dict[\"2-2\"], equity_dict[\"2-3\"], spy_ref],\n",
    "        axis=1\n",
    "    )\n",
    "    all_df.columns = [\"model_2_0\", \"model_2_1\", \"model_2_2\", \"model_2_3\", \"SPY\"]\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def plot_reward_vs_time(all_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Make 3 plots:\n",
    "      - last quarter (approx 63 days)\n",
    "      - last year   (approx 252 days)\n",
    "      - last 5 years (approx 1260 days)\n",
    "    \"\"\"\n",
    "    all_df = all_df.dropna()\n",
    "\n",
    "    def plot_window(ax, df_window, title):\n",
    "        ax.plot(df_window.index, df_window[\"model_2_0\"], label=\"Model 2-0 (HMM once)\")\n",
    "        ax.plot(df_window.index, df_window[\"model_2_1\"], label=\"Model 2-1 (HMM rolling)\")\n",
    "        ax.plot(df_window.index, df_window[\"model_2_2\"], label=\"Model 2-2 (Local vol)\")\n",
    "        ax.plot(df_window.index, df_window[\"model_2_3\"], label=\"Model 2-3 (Hybrid HMM-Heston)\")\n",
    "        ax.plot(df_window.index, df_window[\"SPY\"],       label=\"SPY benchmark\", linestyle=\"--\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(\"Equity (normalized)\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    n = len(all_df)\n",
    "    quarter = 63\n",
    "    year = 252\n",
    "    five_years = 1260\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 12), sharex=False)\n",
    "\n",
    "    # 1 quarter\n",
    "    if n > quarter:\n",
    "        df_q = all_df.iloc[-quarter:]\n",
    "        plot_window(axes[0], df_q, \"Reward vs Time (Last Quarter)\")\n",
    "\n",
    "    # 1 year\n",
    "    if n > year:\n",
    "        df_y = all_df.iloc[-year:]\n",
    "        plot_window(axes[1], df_y, \"Reward vs Time (Last Year)\")\n",
    "\n",
    "    # 5 years\n",
    "    if n > five_years:\n",
    "        df_5y = all_df.iloc[-five_years:]\n",
    "        plot_window(axes[2], df_5y, \"Reward vs Time (Last 5 Years)\")\n",
    "\n",
    "    axes[0].legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    price_path = r\"the file path for stock price\"\n",
    "    options_path = r\"the file path for option\"\n",
    "\n",
    "    all_results = run_all_models(price_path, options_path)\n",
    "    plot_reward_vs_time(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce18e7e-5b92-4ae1-94d9-3c23835907c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d356b-785d-4ce9-8855-ce4ea1c1ee75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
